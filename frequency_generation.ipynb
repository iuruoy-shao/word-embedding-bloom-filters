{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/bloom-filters/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Yourui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Yourui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "data = datasets.load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "# data = datasets.load_dataset(\"bookcorpus/bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokenized = word_tokenize(sentence.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return [lemmatizer.lemmatize(token) for token in tokenized if token not in en_stopwords and wordnet.synsets(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'political',\n",
       " 'philosophy',\n",
       " 'movement',\n",
       " 'sceptical',\n",
       " 'authority',\n",
       " 'reject',\n",
       " 'involuntary',\n",
       " 'coercive',\n",
       " 'form',\n",
       " 'hierarchy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sent_tokenize(next(iter(data['train']))['text'].lower())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import numpy as np\n",
    "\n",
    "def generate_frequencies(word, n_occurrences=10000, deltas=None):\n",
    "    if deltas is None:\n",
    "        deltas = [-4, -3, -2, -1, 1, 2, 3, 4]\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "    frequencies = {}\n",
    "    occurrences = 0\n",
    "\n",
    "    for i, row in enumerate(data['train']):\n",
    "        sentences = sent_tokenize(row['text'].lower())\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if word in sentence:\n",
    "                tokenized = tokenize(sentence)\n",
    "                indices = [i for i, x in enumerate(tokenized) if x == word]\n",
    "                for index in indices:\n",
    "                    for delta in deltas:\n",
    "                        with contextlib.suppress(IndexError):\n",
    "                            try:\n",
    "                                frequencies[tokenized[index + delta]] += 1\n",
    "                            except KeyError:\n",
    "                                frequencies[tokenized[index + delta]] = 1\n",
    "\n",
    "                    occurrences += 1\n",
    "                    if occurrences >= n_occurrences:\n",
    "                        return frequencies\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'\"{word}\", {i}th row processed, {occurrences}/{n_occurrences} occurrences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the most frequently occurring words next to the target word (ex: 1000+ occurrences) + aggregate those words + collect data\n",
    "    # Table 1: Most frequent words\n",
    "    # Table 2: Most frequent words with count vectors\n",
    "# test discarding bits with high variance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store_encoding(word, fname, args):\n",
    "    frequencies = generate_frequencies(word, **args)\n",
    "    \n",
    "    with open(fname, 'r') as f:\n",
    "        encodings = json.load(f)\n",
    "    encodings[word] = frequencies\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(encodings, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"man\", 0th row processed, 0/20000 occurrences\n",
      "\"man\", 1000th row processed, 877/20000 occurrences\n",
      "\"man\", 2000th row processed, 1544/20000 occurrences\n",
      "\"man\", 3000th row processed, 2304/20000 occurrences\n",
      "\"man\", 4000th row processed, 3101/20000 occurrences\n",
      "\"man\", 5000th row processed, 3978/20000 occurrences\n",
      "\"man\", 6000th row processed, 4911/20000 occurrences\n",
      "\"man\", 7000th row processed, 5798/20000 occurrences\n",
      "\"man\", 8000th row processed, 7266/20000 occurrences\n",
      "\"man\", 9000th row processed, 8036/20000 occurrences\n",
      "\"man\", 10000th row processed, 8912/20000 occurrences\n",
      "\"man\", 11000th row processed, 9575/20000 occurrences\n",
      "\"man\", 12000th row processed, 10353/20000 occurrences\n",
      "\"man\", 13000th row processed, 11170/20000 occurrences\n",
      "\"man\", 14000th row processed, 11840/20000 occurrences\n",
      "\"man\", 15000th row processed, 12723/20000 occurrences\n",
      "\"man\", 16000th row processed, 13811/20000 occurrences\n",
      "\"man\", 17000th row processed, 14777/20000 occurrences\n",
      "\"man\", 18000th row processed, 14926/20000 occurrences\n",
      "\"man\", 19000th row processed, 15325/20000 occurrences\n",
      "\"man\", 20000th row processed, 16000/20000 occurrences\n",
      "\"man\", 21000th row processed, 16369/20000 occurrences\n",
      "\"man\", 22000th row processed, 16921/20000 occurrences\n",
      "\"man\", 23000th row processed, 17728/20000 occurrences\n",
      "\"man\", 24000th row processed, 18508/20000 occurrences\n",
      "\"man\", 25000th row processed, 19101/20000 occurrences\n",
      "\"man\", 26000th row processed, 19874/20000 occurrences\n",
      "\"woman\", 0th row processed, 0/20000 occurrences\n",
      "\"woman\", 1000th row processed, 343/20000 occurrences\n",
      "\"woman\", 2000th row processed, 733/20000 occurrences\n",
      "\"woman\", 3000th row processed, 1019/20000 occurrences\n",
      "\"woman\", 4000th row processed, 1385/20000 occurrences\n",
      "\"woman\", 5000th row processed, 1749/20000 occurrences\n",
      "\"woman\", 6000th row processed, 2203/20000 occurrences\n",
      "\"woman\", 7000th row processed, 2583/20000 occurrences\n",
      "\"woman\", 8000th row processed, 3017/20000 occurrences\n",
      "\"woman\", 9000th row processed, 3448/20000 occurrences\n",
      "\"woman\", 10000th row processed, 3991/20000 occurrences\n",
      "\"woman\", 11000th row processed, 4343/20000 occurrences\n",
      "\"woman\", 12000th row processed, 4694/20000 occurrences\n",
      "\"woman\", 13000th row processed, 5097/20000 occurrences\n",
      "\"woman\", 14000th row processed, 5569/20000 occurrences\n",
      "\"woman\", 15000th row processed, 5952/20000 occurrences\n",
      "\"woman\", 16000th row processed, 6405/20000 occurrences\n",
      "\"woman\", 17000th row processed, 6935/20000 occurrences\n",
      "\"woman\", 18000th row processed, 7082/20000 occurrences\n",
      "\"woman\", 19000th row processed, 7257/20000 occurrences\n",
      "\"woman\", 20000th row processed, 7611/20000 occurrences\n",
      "\"woman\", 21000th row processed, 7764/20000 occurrences\n",
      "\"woman\", 22000th row processed, 8024/20000 occurrences\n",
      "\"woman\", 23000th row processed, 8560/20000 occurrences\n",
      "\"woman\", 24000th row processed, 8828/20000 occurrences\n",
      "\"woman\", 25000th row processed, 9080/20000 occurrences\n",
      "\"woman\", 26000th row processed, 9444/20000 occurrences\n",
      "\"woman\", 27000th row processed, 9698/20000 occurrences\n",
      "\"woman\", 28000th row processed, 10010/20000 occurrences\n",
      "\"woman\", 29000th row processed, 10311/20000 occurrences\n",
      "\"woman\", 30000th row processed, 10624/20000 occurrences\n",
      "\"woman\", 31000th row processed, 10883/20000 occurrences\n",
      "\"woman\", 32000th row processed, 11285/20000 occurrences\n",
      "\"woman\", 33000th row processed, 11580/20000 occurrences\n",
      "\"woman\", 34000th row processed, 11925/20000 occurrences\n",
      "\"woman\", 35000th row processed, 12215/20000 occurrences\n",
      "\"woman\", 36000th row processed, 12585/20000 occurrences\n",
      "\"woman\", 37000th row processed, 12985/20000 occurrences\n",
      "\"woman\", 38000th row processed, 13241/20000 occurrences\n",
      "\"woman\", 39000th row processed, 13503/20000 occurrences\n",
      "\"woman\", 40000th row processed, 13801/20000 occurrences\n",
      "\"woman\", 41000th row processed, 14097/20000 occurrences\n",
      "\"woman\", 42000th row processed, 14399/20000 occurrences\n",
      "\"woman\", 43000th row processed, 14653/20000 occurrences\n",
      "\"woman\", 44000th row processed, 14822/20000 occurrences\n",
      "\"woman\", 45000th row processed, 15094/20000 occurrences\n",
      "\"woman\", 46000th row processed, 15205/20000 occurrences\n",
      "\"woman\", 47000th row processed, 15400/20000 occurrences\n",
      "\"woman\", 48000th row processed, 15694/20000 occurrences\n",
      "\"woman\", 49000th row processed, 15979/20000 occurrences\n",
      "\"woman\", 50000th row processed, 16098/20000 occurrences\n",
      "\"woman\", 51000th row processed, 16304/20000 occurrences\n",
      "\"woman\", 52000th row processed, 16370/20000 occurrences\n",
      "\"woman\", 53000th row processed, 16433/20000 occurrences\n",
      "\"woman\", 54000th row processed, 16507/20000 occurrences\n",
      "\"woman\", 55000th row processed, 16564/20000 occurrences\n",
      "\"woman\", 56000th row processed, 16899/20000 occurrences\n",
      "\"woman\", 57000th row processed, 16930/20000 occurrences\n",
      "\"woman\", 58000th row processed, 17042/20000 occurrences\n",
      "\"woman\", 59000th row processed, 17079/20000 occurrences\n",
      "\"woman\", 60000th row processed, 17128/20000 occurrences\n",
      "\"woman\", 61000th row processed, 17192/20000 occurrences\n",
      "\"woman\", 62000th row processed, 17205/20000 occurrences\n",
      "\"woman\", 63000th row processed, 17229/20000 occurrences\n",
      "\"woman\", 64000th row processed, 17247/20000 occurrences\n",
      "\"woman\", 65000th row processed, 17261/20000 occurrences\n",
      "\"woman\", 66000th row processed, 17300/20000 occurrences\n",
      "\"woman\", 67000th row processed, 17355/20000 occurrences\n",
      "\"woman\", 68000th row processed, 17396/20000 occurrences\n",
      "\"woman\", 69000th row processed, 17508/20000 occurrences\n",
      "\"woman\", 70000th row processed, 17545/20000 occurrences\n",
      "\"woman\", 71000th row processed, 17622/20000 occurrences\n",
      "\"woman\", 72000th row processed, 17676/20000 occurrences\n",
      "\"woman\", 73000th row processed, 17714/20000 occurrences\n",
      "\"woman\", 74000th row processed, 17740/20000 occurrences\n",
      "\"woman\", 75000th row processed, 17754/20000 occurrences\n",
      "\"woman\", 76000th row processed, 17808/20000 occurrences\n",
      "\"woman\", 77000th row processed, 17866/20000 occurrences\n",
      "\"woman\", 78000th row processed, 17906/20000 occurrences\n",
      "\"woman\", 79000th row processed, 17967/20000 occurrences\n",
      "\"woman\", 80000th row processed, 17996/20000 occurrences\n",
      "\"woman\", 81000th row processed, 18015/20000 occurrences\n",
      "\"woman\", 82000th row processed, 18027/20000 occurrences\n",
      "\"woman\", 83000th row processed, 18103/20000 occurrences\n",
      "\"woman\", 84000th row processed, 18293/20000 occurrences\n",
      "\"woman\", 85000th row processed, 18675/20000 occurrences\n",
      "\"woman\", 86000th row processed, 18907/20000 occurrences\n",
      "\"woman\", 87000th row processed, 19149/20000 occurrences\n",
      "\"woman\", 88000th row processed, 19284/20000 occurrences\n",
      "\"woman\", 89000th row processed, 19522/20000 occurrences\n",
      "\"woman\", 90000th row processed, 19767/20000 occurrences\n",
      "\"king\", 0th row processed, 0/20000 occurrences\n",
      "\"king\", 1000th row processed, 2093/20000 occurrences\n",
      "\"king\", 2000th row processed, 3448/20000 occurrences\n",
      "\"king\", 3000th row processed, 4847/20000 occurrences\n",
      "\"king\", 4000th row processed, 5952/20000 occurrences\n",
      "\"king\", 5000th row processed, 7441/20000 occurrences\n",
      "\"king\", 6000th row processed, 8712/20000 occurrences\n",
      "\"king\", 7000th row processed, 11198/20000 occurrences\n",
      "\"king\", 8000th row processed, 12931/20000 occurrences\n",
      "\"king\", 9000th row processed, 14530/20000 occurrences\n",
      "\"king\", 10000th row processed, 16510/20000 occurrences\n",
      "\"king\", 11000th row processed, 18050/20000 occurrences\n",
      "\"king\", 12000th row processed, 19435/20000 occurrences\n",
      "\"queen\", 0th row processed, 0/20000 occurrences\n",
      "\"queen\", 1000th row processed, 444/20000 occurrences\n",
      "\"queen\", 2000th row processed, 802/20000 occurrences\n",
      "\"queen\", 3000th row processed, 1155/20000 occurrences\n",
      "\"queen\", 4000th row processed, 1460/20000 occurrences\n",
      "\"queen\", 5000th row processed, 1999/20000 occurrences\n",
      "\"queen\", 6000th row processed, 2343/20000 occurrences\n",
      "\"queen\", 7000th row processed, 2817/20000 occurrences\n",
      "\"queen\", 8000th row processed, 3264/20000 occurrences\n",
      "\"queen\", 9000th row processed, 3526/20000 occurrences\n",
      "\"queen\", 10000th row processed, 3921/20000 occurrences\n",
      "\"queen\", 11000th row processed, 4384/20000 occurrences\n",
      "\"queen\", 12000th row processed, 4699/20000 occurrences\n",
      "\"queen\", 13000th row processed, 5004/20000 occurrences\n",
      "\"queen\", 14000th row processed, 5351/20000 occurrences\n",
      "\"queen\", 15000th row processed, 5602/20000 occurrences\n",
      "\"queen\", 16000th row processed, 5952/20000 occurrences\n",
      "\"queen\", 17000th row processed, 6411/20000 occurrences\n",
      "\"queen\", 18000th row processed, 6945/20000 occurrences\n",
      "\"queen\", 19000th row processed, 7437/20000 occurrences\n",
      "\"queen\", 20000th row processed, 8084/20000 occurrences\n",
      "\"queen\", 21000th row processed, 8602/20000 occurrences\n",
      "\"queen\", 22000th row processed, 9294/20000 occurrences\n",
      "\"queen\", 23000th row processed, 9795/20000 occurrences\n",
      "\"queen\", 24000th row processed, 10599/20000 occurrences\n",
      "\"queen\", 25000th row processed, 11554/20000 occurrences\n",
      "\"queen\", 26000th row processed, 12413/20000 occurrences\n",
      "\"queen\", 27000th row processed, 12803/20000 occurrences\n",
      "\"queen\", 28000th row processed, 13062/20000 occurrences\n",
      "\"queen\", 29000th row processed, 13366/20000 occurrences\n",
      "\"queen\", 30000th row processed, 13901/20000 occurrences\n",
      "\"queen\", 31000th row processed, 14196/20000 occurrences\n",
      "\"queen\", 32000th row processed, 14572/20000 occurrences\n",
      "\"queen\", 33000th row processed, 14949/20000 occurrences\n",
      "\"queen\", 34000th row processed, 15265/20000 occurrences\n",
      "\"queen\", 35000th row processed, 15855/20000 occurrences\n",
      "\"queen\", 36000th row processed, 16258/20000 occurrences\n",
      "\"queen\", 37000th row processed, 16711/20000 occurrences\n",
      "\"queen\", 38000th row processed, 17079/20000 occurrences\n",
      "\"queen\", 39000th row processed, 17504/20000 occurrences\n",
      "\"queen\", 40000th row processed, 17753/20000 occurrences\n",
      "\"queen\", 41000th row processed, 18028/20000 occurrences\n",
      "\"queen\", 42000th row processed, 18283/20000 occurrences\n",
      "\"queen\", 43000th row processed, 18615/20000 occurrences\n",
      "\"queen\", 44000th row processed, 18695/20000 occurrences\n",
      "\"queen\", 45000th row processed, 18961/20000 occurrences\n",
      "\"queen\", 46000th row processed, 19096/20000 occurrences\n",
      "\"queen\", 47000th row processed, 19226/20000 occurrences\n",
      "\"queen\", 48000th row processed, 19557/20000 occurrences\n",
      "\"queen\", 49000th row processed, 19935/20000 occurrences\n"
     ]
    }
   ],
   "source": [
    "l = [\"man\",\"woman\",\"king\",\"queen\"]\n",
    "\n",
    "for value in l:\n",
    "    store_encoding(value, 'data/wikipedia_20000_frequencies.json', {'n_occurrences':20000, 'deltas': [-4, -3, -2, -1, 1, 2, 3, 4]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
